\section{Game Theory}
Game theory is a branch of applied mathematics that derives mathematical models to predict the outcome of competitive interactions between two or more rational decision makers. A game may involve: 
\begin{itemize}
\item Common interest (coordination);
\item Competing interests (rivalry);
\item Rational behavior: players can do the best they can, in their own eyes;
\item a rational decision in a game must be based on a prediction of others' responses;
\end{itemize}
\clearpage
 

\subsection{Defining Games} 
A game is the interaction between rational players, where the decisions of some players changes the payoff of others. A game consists of four parts : 
Players, Actions, strategies and Payoffs.
\begin{itemize}
\item Players are the decision makers and they can be : People, Governments or Companies;
\item Actions are decisions that the players make;
\item Strategies are composed of actions;
\item Payoffs are the outcomes which players receive as a result of their decisions and those of their opponents;
\end{itemize}
Games can be represented using two methods : Normal forms and Extensive Form.
\subsubsection{Extensive Form}
An extensive form game includes timing of moves. 
Players move sequentially, represented as a tree. These are examples of games that can be represented in the extensive form: 
\begin{itemize}
\item Chess: white player moves, then black player can see white's move and react...
\end{itemize}
Keeps track of what each player knows when he or she makes a decision :
\begin{itemize}
\item Poker: bet sequentially - what can a given player see when they bet. 
\end{itemize}

\subsubsection{Normal Form Games}
A normal form game is a strategic interaction in which each of $n$ players chooses a strategy and the receives a payoff that depends on all agents choices of strategy. In other words, a normal form represents a list of what players get on function of their actions (Jackson, Leyton-Brown and Shoham; 2013)
Finite, n-person normal form game  ⟨$N, A, u$⟩:
\begin{itemize}
\item Players: $ N = {1, ... , n} $ is a finite set of $n$, indexed by $i$.
\item Actions set for player $i$ $A_i$
\subitem $a = (a_1,...,a_n) \in A = A_1 * ... * A_n $ is an action profile.
\item Utility function or Payoff function for player $i: u_i : A $  $\to$ ${\Bbb{R}}$
\subitem $u = (u_1,..., u_n)$, is a profile of utility functions.
\end{itemize}

\subsubsection{Nash Equilibrium}\label{subsection}
A Nash Equilibrium specifies that the optimal outcome of a game is one from which no player can benefit by changing his strategy if none of his opponents do so as well. Nash Equilibrium is reached  over time, in most cases. However, these different choices over time before reaching an equilibrium is often played out in the business world when two firms are determining prices for products.

\begin{mydef}[Nash Equilibrium]\label{def:def1500}
Nash equilibrium is the profile of actions such that each action is a best response to the other actions,
$a = <a_1,...,a_n>$ is a pure strategy Nash equilibrium if $\forall i, a_i \in BR(a_{-i})$.
\end{mydef}

\subsubsection{Dominant strategies}
A strategy is called dominant if regardless of what any other players do, the strategy earns the player a larger payoff than the others. Consider $s_i$ and $s_i^`$ to be two strategies for player $i$, and $S_{-i}$ be the set of all possible strategy profiles for other players (Jackson, Leyton-Brown and Shoham; 2013).
\begin{itemize}
\item A strategy is strictly dominant if regardless of what any other players do, the strategy earns the player a strictly higher payoff than any other. If one strategy is strictly dominant, than all others are dominated. $s_i$ strictly dominates $s^\prime_{i}$ if $\forall s_{-i} \in S_{-i}, u_i(s_i, s_{-i}) \>> u_i(s^\prime_{i}, s_{-i})$;
\item A strategy is weakly dominant if regardless of what other players do, the strategy earns a player a payoff at least as high as any other strategy, and the strategy earns a strictly higher payoff than other players;
\item A strategy profile consisting of dominant strategies for every player must be a Nash Equilibrium;
\end{itemize}

\subsection{Mixed Strategies and Nash Equilibrium}
A mixed strategy consists of possible move and a probability distribution which corresponds to how frequently each move is to be played.
\title{\textbf{Definition:}}
A strategy $s_i$ for agent $i$ as any probability distribution over the actions $A_i$.
\begin{itemize}
\item \textbf{pure strategy:} only one action is played with positive probability;
\item \textbf{mixed strategy:} more than one action is played with positive probability, these actions are called the support of the mixed strategy;
\end{itemize}

\subsubsection{Utility in Mixed Strategies}
In order to find the payoff, if all the players follow mixed strategy profile $s \in S$ we can use the expected utility from decision theory as in equations (1.1) and (1.2): 
\begin{equation} \label{eq:112}
u_i(s) = \sum_{a \in A}u_i(a)P(a|s)
\end{equation}
\begin{equation} 
P(a|s) = \prod_{j \in N}s_j(a_j)
\end{equation}
\subsubsection{Best Response and Nash Equilibrium}The definitions of best response and Nash equilibrium using mixed strategy are generalized from actions to strategies. 
\begin{mydef}[Best response]\label{def:def77} The best response for a player is the strategy of that generate the greatest payoff.
\begin{equation}\label{eq:111}
s_i^* \in BR(s_{-i}) \textbf{ if }   \forall s_i \in S_i, u_i(s_i^*,s_{-i}) \geq u_i(s_i, s_{-i})
\end{equation}
\end{mydef}
\begin{mydef}[Nash Equilibrium]\label{def:def88} The expected payoff must be at least as large as that obtainable by any other strategy:
$$s = <s_1,...,s_n>\textbf{ is  a }\textbf{Nash Equilibrium if } \forall i, s_i \in BR(s_{-i})$$
\end{mydef}

\paragraph{Theorem (Nash, 1950)} Every finite game has a Nash equilibrium.

\subsubsection{Computing Nash Equilibrium}
Computing a Nash equilibrium is classified to be a PPAD-complete \footnote{PPAD : Polynomial Parity Argument on Directed Graphs}. The first algorithm is known to be the complementary pivot algorithm as a solution to the linear complementary problem\footnote{Linear complementary algorithm is a general problem that unifies linear programming, quadratic programming and bi-matrix games}, which was developed by Lemke and Howson. The algorithm was later generalized for noncooperative n-person games by  Rosenmuller. The idea of the Lemke-Howson algorithm is to perform pivoting steps between the vertices of a polytope related to the game until a Nash equilibrium is found. The issue with these algorithms is that their worst case running time is exponential to the number of players and the size of the strategy sets.
\subsubsection{Perfect information games}
A game is called a perfect information game if only one player moves at a time and if each player knows every action of the players that moved before him at every point in the game.
The extensive form is an alternative representation that makes the temporal structure explicit. A finite perfect information game in extensive form is defined by the tuple ($N, A, H, Z,\chi ,\rho, \sigma, u $)
where:
\begin{itemize}
\item{Players: $N$ is a set of $n$ players.}
\item{Actions: $A$ is a set of actions.}
\item{Choice nodes and labels for these nodes: }
\begin{itemize}
\item{Choice nodes: $H$ is a set of non-terminal choice nodes.}
\item{Action function: $\chi : H \to 2^A $ assigns to each choice a set of actions.}
\item{Player function: $\rho : H \to N$ assigns to each non-terminal node $h$ a player $i \in N$ who chooses an action at $h$.}
\end{itemize}
\item{$Z$ is a set of terminal nodes\footnote{Terminal node: a node of a tree data structure that has no child nodes.}, disjoint from $H$.}
\item{Successor function: $\sigma : H \times A \to H \cup Z$ maps a choice node and an action to a new choice node or terminal node such that for all $h_1, h_2 \in H$ and $a_1, a_2 \in A$, if $\sigma(h_1, a_1) = \sigma(h_2, a_2)$ then $h_1  = h_2$  and $a_1 = a_2$} 
\item{Utility function: $u = (u_1,...,u_n)$ where $u_i : Z \to R$.
}
\end{itemize} 
\paragraph{}An example of a game that can be represented in extensive form is the sharing game, where two players try to decide how they can split $2\$$ in between them as shown in figure \ref{fig:sf}.
\begin{figure}[h!]
 
  \centering
  \begin{tikzpicture}[baseline] % baseline makes the example number stay at the top of the tree
   \Tree[.1 [.\textit{2 } [.\textit{(0,0) } ] [.\textit{(2,0) } ]][.\textit{2 } [.\textit{(0,0) } ] [.\textit{(1,1) } ]][.\textit{2 } [.\textit{(0,0) } ] [.\textit{(0,2) } ]]]
     \end{tikzpicture}%
  \caption{Sharing Game\label{fig:sf}}
\end{figure}
\subsubsection{Pure Strategies}
\paragraph{} A pure strategy for a player in a perfect-information game is a complete specification of which action to take at each node belonging to that player.
\paragraph{Definition} Let $G = (N, A, H, Z,\chi ,\rho, \sigma, u ) $ be a perfect-information extensive-form game. Then the pure strategies of player $i$ consist of the cross product\\
\begin{equation}
\prod_{h \in H, \rho(h)=i}\chi(h)
\end{equation}
Given our new definition of pure strategy, we can reuse our old definitions of mixed strategies and Nash equilibrium defined in 1.1 through 1.4.
\subsubsection{Sub-game Perfection}
A subgame Nash equilibrium is an equilibrium such that the strategies of players constitute a Nash equilibrium in each subgame of the game. It may be found by backwards induction.
\begin{mydef}[Sub-game Perfection]\label{def:def555}
The set of sub-games of $G$ is defined by the sub-games of $G$ rooted at each of the nodes in $G$.
\end{mydef}
\paragraph{}Let $s$ be a  sub-game perfect equilibrium of $G$ if for any sub-game $G'$ of $G$, the restriction of $s$ to G' is a Nash equilibrium of $G'$. Since $G$ is its own sub-game , every sub-game perfect is a Nash equilibrium.

\subsubsection{Backward Induction}
Backward induction is an iterative process for solving finite extensive form games. First, one determines the optimal strategy of the player who makes the last move of the game. Then, the optimal action of the next to last moving player is determined taking the last player's action as given. The process continues in this way backwards in time until the actions have been determined.

\begin{algorithm}[h!]
\caption{Backward Induction\label{fig:scaled_back}}
\begin{algorithmic}
 \STATE Input : node h
 \STATE Output : $u(h)$
\IF{$h \in Z$}  
\RETURN{$u(h)$}   \COMMENT{return the payoff vector if the node h is a leaf node}
\ENDIF
\STATE best-util $\leftarrow -\infty$ \COMMENT {best-util is a payoff vector associated with each agent}
\FORALL{$a \in \rho(h)$} 
\STATE $utc \leftarrow BACKWARDINDUCTION(\sigma(h,a))$  \COMMENT{look at all actions available from node h, where $\sigma(h,a)$ is the child node arrived at by taking action $a$ }
\IF{$utc_{\rho_{(h)}} > best-util_{\rho_{(h)}}$}
\STATE best-util $\leftarrow utc$ \COMMENT {update the best utility vector if the utility at the child node is better}
\ENDIF
\ENDFOR
\RETURN best-util
\end{algorithmic}
\end{algorithm}
\paragraph{}Backward Induction has been used in solving games since John von Neumann and Oskar Morgenstern published their book, Theory of Games and Economic Behavior in 1944 (Von Neumann and Morgenstern; 1944). note that $utc$ or "utility at child" is a utility vector for each player (Jackson, Leyton-Brown and Shoham, 2013). Effectively, one determines the Nash Equilibrium of each subgame of the game, as shown in algorithm \ref{fig:scaled_back}.
\\

\subsection{Repeated Games}
So far we've payed attention to one stage games, that  is, games in which players concerns do not extend than beyond the first stage interaction. However, games are often played with a futuristic mindset, and this can significantly change their outcomes and equilibrium strategies. The topic of this section is repeated games, that is, games in which players face similar situations on multiple occasions.
\begin{mydef}[Average Utilities]\label{def:def666}
Given an infinite sequence of payoffs $(r_1, r_2, ...) $ for player $i$, the average reward of $i$ is
 \begin{equation}
  \lim_{x \to +\infty} \sum_{j=1}^k \frac{r_j}{k}
 \end{equation}	
\end{mydef}
\begin{mydef}[Discounted Utilities]\label{def:def777}
Given an infinite sequence of payoffs  $(r_1, r_2, ...) $ for player $i$ and a discount factor $\beta$\ with $ 0 < \beta < 1 $, the corresponding future discount reward is: 
 \begin{equation}\label{eq:777}
 \sum_{j=1}^{+\infty} \beta^j r_j
\end{equation}
\end{mydef}
\paragraph{}In \ref{eq:777}, the players care about the future $j$ just as much as the present, but with a probability of $1 - \beta$ the game will end in any round.
\begin{mydef}[Equilibrium of Infinitely Repeated Games]\label{def:def888}
Consider any $n$-player game $G = (N,A,u)$ and a payoff vector $r = (r_1, r_2, ..., r_n)$
 \begin{equation}\label{eq:999}
	v_i = \min_{S_{-i} \in S_{-i}} \max_{S_i \in S} u_i (S_{-i}, S_i)
\end{equation}
\end{mydef}
\paragraph{} The minmax value of the player $i$ in equation \ref{eq:999} is the amount of utility $i$ can get when the other player $-i$ plays a minmax strategy against him. For example, the minmax payoff in a Prisoner's Dilemma  is (1,1) as shown in table \ref{table:7}.
\begin{table}[h!]
\centering
\begin{tabular}{lllll}

\multicolumn{1}{l}{1/2} \vline  & \multicolumn{1}{l}{w} & \multicolumn{1}{l}{s}  \\ 
\hline
w                          & 2,2                                  & 0,3				
\\
s                          &3,0                                    & 1,1                                     
                                      
\end{tabular}
\caption{Prisoner's Dilemma}
\label{table:7}
\end{table}

\begin{mydef}[Folk Theorem]
Consider a finite normal form game $G = (N, A, u)$, and $a = (a_1, ...,a_n )$ to be a Nash equilibrium of the stage game $G$. If $a^\prime = (a^\prime_1, ..., a^\prime_n)$ is such that $u_i(a^\prime)> u_i(a)$ for all $i$, then there exists a discount factor $\beta <1$, such that if $\beta_i \geq \beta$ for all $i$, then there exists a subgame perfect Nash equilibrium of the infinite repetition of $G$ that has $a^\prime$ played in every period on the equilibrium path.
\end{mydef}
\subsection{Population Games}
\paragraph{}Population games provide a simple and general framework for studying strategic interactions in large populations whose members play pure strategies. The simplest population games are generated by random matching in normal form games, but the population game framework allows for interactions of a more intricate nature.
\paragraph{}Focusing on games played by a single population, all players in this game play equivalent roles. Suppose that there is a unit mass of players, each of whom chooses a pure strategy from the set
 $S = {1, ... , n}$. The aggregate behavior of these players is described by a population state $x \in X$, with $x_j$ representing the proportion of agents choosing pure strategy $j$. We identify a population game with a continuous vector valued payoff function $ F:X \rightarrow R^n$. The scalar $F(x)$ represents the payoff of a strategy $i$ when the population state is $x$. Population state $x^*$ is a Nash equilibrium of $F$ if no player can improve his payoff by unilaterally switching strategies.
\subsection{The lack of dynamics in traditional game theory}
Von Neuman and Morgenstern mentioned that the theory of games is static, and a dynamic theory would be more fitted and preferable (The theory of games, V.Neuman and Morgenstern; 1953). At the time of the publication, evolutionary game theory was unknown.
\paragraph{}
In order to capture the dynamics of a decision making process, it is preferable to represent the game in the extensive form rather than normal form for games of a normal complexity. However, extensive form games tend to get complex and difficult to manage. Traditional game theory represents a set of strategies for a player at each stage of the game. This representation lacks the element of learning in players when encountering similar choices which leads to better decision making. This shows the incompetence of traditional game theory in the dynamics model of decision making. Therefore, evolutionary game theory incorporates dynamic factors.
\section{Evolutionary Game Theory}

Evolutionary Game Theory was introduced by John Maynard Smith in Evolution and The Theory of Games in 1982. The theory was formulated to understand the behavior of animals in game theoretic situations. But it can be applied to modeling human behavior.

\paragraph{}After the emergence of traditional game theory, biologists realized the potential of game theory to formally study adaptation of biological populations, especially in contexts where the fitness of a phenotype depends on the composition of the population (Hamilton, 1967). The main assumption of evolutionary game theory was that strategies with greater payoffs at a particular time would tend to spread more and thus have better chances of being present in the future.
\paragraph{}The most important concept of evolutionary thinking that was introduced by Manynard Smith and Price (1973) is the notion of Evolutionary Stable Strategy (ESS), for 2-player symmetric games played by individuals belonging to the same population. Furthermore, a strategy $s$ is an ESS if and only if, when adopted by all members of a population, meaning that any other strategy $i$ that could enter the population in a low percentage would obtain a strictly  lower expected payoff in the population than the $s$ strategy.
\paragraph{}The basic ideas behind Evolutionary game theory is that strategies with greater payoffs tend to spread more, and that fitness is frequency dependent soon transcended the borders of biology and started to spread through many other disciplines. In economic context, it was understood that natural selection would derive from competition among entities for small resources or market shares. In social contexts, evolution was often understood as cultural evolution, and it referred to dynamic changes in behavior or ideas over time (Nelson and Winter, 1982)(Boyd and Richerson, 1985).
\paragraph{}In order to extend this understanding further, let's consider this example:
Suppose that a small group of mutants choosing a strategy different from $\delta$* to enter the population.
\begin{itemize}
\item Denote the fraction of mutants in the population by $\varepsilon$ and assume that the mutant adopts the strategy $\delta$.
\item The expected payoff of a mutant is : 
	$(1-\varepsilon)u(\delta,\delta*)+\varepsilon u(\delta*,\delta)$
\item The expected payoff of a mutant that adopts the strategy is :	\\
	$(1-\varepsilon)u(\delta*,\delta*)+\varepsilon u(\delta*,\delta)$
	\item For any mutation to be driven out of the population we need the expected payoff of any mutant to be less than the expected payoff of normal organism :\\
	\begin{equation}(1-\varepsilon)u(\delta*,\delta*)+\varepsilon u(\delta*,\delta) > (1-\varepsilon)u(\delta,\delta*)+\varepsilon u(\delta*,\delta)  \end{equation}
\end{itemize}
\subsection{Static Notions of Evolutionary Stability}
Maynard Smith offered a stability concept for populations of animals sharing a common behavioral trait, that of player a mixed strategy in  the game. Maynard defines such a population as stable if it is resistant to invasion by a small group of mutants carrying a different strategy(Sandholm, 2017).
\paragraph{}Suppose that a large population is randomly matched to play the symmetric normal form game $A$. We call a mixed strategy $x \in X$ an \textbf{evolutionarily stable strategy} (ESS) if 
\begin{equation}\label{eq:55}
x' A((1 - \epsilon)x + \epsilon y) > y' A((1 - \epsilon)x + \epsilon y) 
\end{equation}
\begin{center}
$\forall \epsilon \leq \epsilon(y)$ and $y \neq x.$
\end{center}
\paragraph{}In order to explain condition \ref{eq:55}, let's consider a population programmed to play mixed strategy $x$ is invaded by a small group of mutants programmed to play the alternative mixed strategy $y$. Equation \ref{eq:55} requires that regardless of the choice of $y$, an individual's expected payoff from a random match in the post entry population exceeds that of a mutant so long as the size of the invading group is sufficiently small.
\paragraph{}Maynard Smith's notion of ESS attempts to capture the dynamic process of natural selection using a static definition.


